\section{Метод максимального правдоподобия}
Пусть $\{x_n\}$ -- случайная выборка из генеральной совокупности с плотностью вероятности $f(x, \theta)$, где $\theta \in \mathbb{R}^m, m \in \mathbb{N}$ -- совокупность параметров плотности вероятности.

\begin{definition}
	\textbf{Функцией правдоподобия} назовём совместную плотность вероятность независимых случайных величин $x_1, ..., x_n$ с \textit{одним и тем же} параметром распределения $\theta$:
	\begin{equation}
	L(x_1, ..., x_n, \theta) = f(x_1, \theta) \cdot ... \cdot f(x_n, \theta)
	\end{equation}
\end{definition}

\begin{definition}
	\textbf{Оценкой максимального правдоподобия} $\hat{\theta_{мп}}$ назовём такое значение параметра, при котором функция правдоподобия достигает своего максимума.
\end{definition}

\begin{remark}
	$L$ -- функция \textbf{одной переменной} $\theta$. Будем далее иметь в виду, что любые операции дифференцирования ведутся исключительно по $\theta$.
\end{remark}

Если $L$ дважды дифференцируема, то достаточным условием максимума будет равенство нулю градиента и отрицательная определённость якобиана.

Также можно рассматривать не саму функцию правдоподобия, а её натуральный логарифм, поскольку применение внешней монотонной функции не изменяет точек экстремума. Например, это удобно, когда мы имеем дело с нормальным распределением, так как в нём фигурирует экспоненциальная зависимость, которая может быть линеаризована посредством применения натурального логарифма.

\section{Проверка гипотезы о законе генеральной совокупности. Метод $\chi^2$}

Для проверки гипотезы о функции распределения часто используется критерий согласия $\chi^2$.

Для одномерного случая, когда функция распределения не содержит неизвестных параметров, методика следующая.

Сделаем разбиение вещественной оси на $k$ полуинтервалов $\Delta_i$. Обозначим через $n_i$ количество событий, попавших в интервал $\Delta_i$.

Для нормального распределения получаем, что выборочное среднее -- о.м.п. матожидания, а выборочная дисперсия -- о.м.п. генеральной дисперсии.
	
Если гипотеза справедлива, то относительные частоты должны быть близки к вероятности. Проверка такой близости производится по $l^2$-норме с весами $c_i=\frac{n}{p_i}$:
	
\begin{equation}
	\chi^2 = \displaystyle \sum_{i=1}^{k} c_i \left( \frac{n_i}{n} - p_i \right)^2
\end{equation}
	
Данное обозначение критерия близости, называемого \textit{статистикой критерия $\chi^2$} неслучайно, поскольку имеет место следующая
	
\begin{theorem} \textbf{Пирсона}.
		Статистика $\chi^2$ асимптотически распределена по закону $\chi^2$ с $k-1$ степенями свободы.
\end{theorem}

То есть, какую бы мы гипотезу ни проверяли, функция распределения статистики стремится к истинной функции распределения случайной величины с плотностью вероятности:
\begin{equation}
	f_{k-1}(x)=
	\begin{cases}
	0, & x \leq 0 \\
	\frac{1}{2^{\frac{k-1}{2}}\Gamma\left({\frac{k-1}{2}}\right)}x^{\frac{k-3}{2}}e^{-\frac{x}{2}}, & x > 0
	\end{cases}
	\end{equation}

Также необходимо понять, что какую гипотезу мы будем считать достоверной, а какую -- нет. Для этого необходимо ввести \textit{уровень значимости} $\alpha$. С его помощью проверка будем производиться следующим образом:

\begin{itemize}
\item Если $\chi_{\text{В}}^2 < \chi_{1 - \alpha}^2(k - 1)$, то гипотеза принимается
\item Иначе гипотеза считается несостоятельной
\end{itemize}