\section{Простая линейная регрессия}
\subsection{Описание модели}

Регрессионную модель описания данных называют простой линейной, если заданный набор данных аппроксимируется прямой с внесённой добавкой в виде некоторой нормально распределённой ошибки:
\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, i \in \overline{1,n}
\end{equation}

где 

$\{x_n\}_{n \in \mathbb{N}}$ -- заданные значения,

$\{y_n\}_{n \in \mathbb{N}}$ -- параметры отклика,

$\{\varepsilon_n\}_{n \in \mathbb{N}}$ -- независимые, центрированные, нормально распределённые случайные величины с неизвестной дисперсией $\delta$, суть предполагаемые погрешности,

$\beta_0, \beta_1$ -- параметры, подлежащие оцениванию.

В данной модели мы считаем, что у заданных значений нет погрешности (пренебрегаем ей). Полагаем, что основная погрешность получается при измерении $\{y_n\}_{n \in \mathbb{N}}$.

\subsection{Метод наименьших квадратов}

Данный метод, вообще говоря, может применяться для аппроксимации заданного набора экспериментальных данных линейной комбинацией линейно независимых функций, размер которой не превосходит мощности множества данных (в случае равенства получаем интерполяцию).

Критерием оптимальности подобранной аппроксимации является $l^2$-норма, точнее, для простоты вычисления её квадрат:
\begin{equation}\label{eq:lsm}
\|\displaystyle \sum_{i=1}^{m} \lambda_if_i(\{x_n\}) - \{y_n\} \|_{l^2}^{2} \underset{\{\lambda_i\}}{\longrightarrow} min
\end{equation}

Минимум ищется исходя из критерия равенства нулю градиента и положительной определённости якобиана.

Для того, чтобы градиент был равен нулю, необходимо решить СЛАУ с матрицей:
\begin{equation*}
\begin{pmatrix}
[f_1(\{x_n\}), f_1(\{x_n\})] & \cdots & [f_1(\{x_n\}), f_m(\{x_n\})] \\
\vdots & \ddots & \vdots \\
[f_m(\{x_n\}), f_1(\{x_n\})] & \cdots & [f_m(\{x_n\}), f_m(\{x_n\})]
\end{pmatrix}
\end{equation*}

и правым столбцом:
\begin{equation*}
\begin{pmatrix}
[f_1(\{x_n\}), \{y_n\}] \\
\vdots\\
[f_m(\{x_n\}), \{y_n\}]
\end{pmatrix}
\end{equation*}

где $[f, g]$ -- стандартное скалярное произведение векторов в ортнормированном базисе в $\mathbb{R}^n$.

Матрица скалярных произведений является матрицей Грама, а значит она невырожденная. Якобиан здесь также является положительной полуопределённой матрицей, значит найденное решение будет точкой минимума.

В нашем случае мы аппроксимируем одной линейной функцией $y(x) = \beta_0 + \beta_1 x$, а получаемый в результате решения задачи минимизации ~\eqref{eq:lsm} результат будет являться суммой квадратов $\varepsilon_i$, иными словами, нормировочным множителем для распределения $\{\varepsilon_i\}$.
