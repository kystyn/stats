\section{Простая линейная регрессия}
\subsection{Описание модели}

Регрессионную модель описания данных называют простой линейной, если заданный набор данных аппроксимируется прямой с внесённой добавкой в виде некоторой нормально распределённой ошибки:
\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, i \in \overline{1,n}
\end{equation}

где 

$\{x_n\}_{n \in \mathbb{N}}$ -- заданные значения,

$\{y_n\}_{n \in \mathbb{N}}$ -- параметры отклика,

$\{\varepsilon_n\}_{n \in \mathbb{N}}$ -- независимые, центрированные, нормально распределённые случайные величины с неизвестной дисперсией $\delta$, суть предполагаемые погрешности,

$\beta_0, \beta_1$ -- параметры, подлежащие оцениванию.

В данной модели мы считаем, что у заданных значений нет погрешности (пренебрегаем ей). Полагаем, что основная погрешность получается при измерении $\{y_n\}_{n \in \mathbb{N}}$.

\subsection{Метод наименьших квадратов}

Данный метод, вообще говоря, может применяться для аппроксимации заданного набора экспериментальных данных линейной комбинацией линейно независимых функций, размер которой не превосходит мощности множества данных (в случае равенства получаем интерполяцию).

Критерием оптимальности подобранной аппроксимации является $l^2$-норма, точнее, для простоты вычисления, её квадрат:
\begin{equation}\label{eq:lsm}
\|\displaystyle \sum_{i=1}^{m} \lambda_i f_i(\{x_n\}) - \{y_n\} \|_{l^2}^{2} \underset{\{\lambda_i\}}{\longrightarrow} min
\end{equation}

Минимум ищется по коэффициентам линейной комбинации, исходя из критерия равенства нулю градиента и положительной определённости якобиана.

Для того, чтобы градиент был равен нулю, необходимо решить СЛАУ с матрицей:
\begin{equation}\label{eq:lsmMatr}
\begin{pmatrix}
[f_1(\{x_n\}), f_1(\{x_n\})] & \cdots & [f_1(\{x_n\}), f_m(\{x_n\})] \\
\vdots & \ddots & \vdots \\
[f_m(\{x_n\}), f_1(\{x_n\})] & \cdots & [f_m(\{x_n\}), f_m(\{x_n\})]
\end{pmatrix}
\end{equation}

и правым столбцом:
\begin{equation}
\begin{pmatrix}
[f_1(\{x_n\}), \{y_n\}] \\
\vdots\\
[f_m(\{x_n\}), \{y_n\}]
\end{pmatrix}
\end{equation}

где $[f, g]$ -- стандартное скалярное произведение векторов в ортнормированном базисе в $\mathbb{R}^n$.

Матрица скалярных произведений является матрицей Грама, а значит она невырожденная. Якобиан здесь также является положительной полуопределённой матрицей, значит найденное решение будет точкой минимума.

В нашем случае мы аппроксимируем одной линейной функцией $y(x) = \beta_0 + \beta_1 x$, а получаемый в результате решения задачи минимизации ~\eqref{eq:lsm} результат будет являться суммой квадратов $\varepsilon_i$, иными словами, нормировочным множителем для распределения $\{\varepsilon_i\}$.

Для данной задачи имеем:
\begin{equation}
\begin{cases}
	\hat{\beta_1} = \frac{\overline{xy} - \overline{x} \cdot \overline{y}}{\overline{x^2}-\overline{x}^2} \\
	
	\hat{\beta_0} = \overline{y} - \overline{x} \hat{\beta_1}
\end{cases}	
\end{equation}

\section{Метод наименьших модулей}

Данный метод основан на минимизации $l^1$-нормы разности последовательностей полученных экспериментальных данных $\{y_n\}$ и значений аппроксимирующей функции $f(\{x_n\})$. Увы, автору данного отчёта неизвестно метода, позволяющего решить, как в случае МНК, данную задачу минимизации для линейной комбинации заданного количества базисных функций, действующих на $\mathbb{R}$, однако метод позволяет решать задачу для линейной функции любой размерности:

\begin{equation}\label{eq:lmm}
	\|[\boldsymbol{a}, \{x_n\}] - \{y_n\} \|_{l^1} \underset{\{\lambda_i\}}{\longrightarrow} min
\end{equation}

Данную задачу минимизации можно решать точно, например, используя алгоритм спуска по узловым направлениям. Метод основан на теореме о том, что точка минимума искомой функции лежит в одной из точек нарушения дифференцируемости минимизируемой функции (в точке, где какой-либо модуль обращается в ноль), заданного данными и реализует направленный перебор всех таких точек \hyperref[tyrsin]{[2]}.

Кроме того, можно решать численно, методом Вейсфельда \hyperref[weysfeld]{[3]}. Суть метода в том, что вместо решения негладкой задачи мы на каждой итерации минимизируем взвешенную $l^2$-норму разности, где вес равен величине, обратной невязке на предыдущем шаге (таким образом, мы делим квадрат невязки на текущем шаге на невязку на предыдущем, и получаем ``почти невязку'' в первой степени, что соответствует $l^1$-норме).

Нетрудно понять, что такую задачу можно решать с помощью МНК -- отличие лишь в том, что скалярные произведения в соответствующей матрице ~\eqref{eq:lsmMatr} будут взвешенными.